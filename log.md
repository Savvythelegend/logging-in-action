We'll follow an **intuitive learning approach**:  
‚úÖ **Step 1:** What is logging & why use it? (Quick theory)  
‚úÖ **Step 2:** Basic logging (Hands-on coding)  
‚úÖ **Step 3:** Advanced logging (Different log levels, handlers, and formatting)  
‚úÖ **Step 4:** Real-world use cases (ML pipeline logging, Flask API logs, file logging)  
‚úÖ **Step 5:** Build a **mini-project** where logging actually helps you üöÄ  

Let‚Äôs start with **Step 1: Understanding Logging** (Quick notes), and then we‚Äôll jump into the hands-on part.  

---

## **üìå Step 1: What is Logging & Why Use It?**
### üîπ **What is Logging?**  
Logging is a way to **track events in a program** while it's running. Instead of using `print()`, we use `logging` to:  
‚úÖ **Debug** without cluttering the console  
‚úÖ **Track errors** in production  
‚úÖ **Save logs** to a file for future analysis  
‚úÖ **Monitor ML model training** (loss, accuracy, data issues)  

### üîπ **Why Not Just Use `print()`?**
‚ùå `print()` is temporary and doesn‚Äôt save logs.  
‚ùå You can‚Äôt control the **log level** (info, warning, error).  
‚ùå Hard to filter logs in a big application.  
‚úÖ Logging is **structured**, saves data, and supports filtering.  

---

### **üìå Step 2: Hands-on - Basic Logging**
Try this in a Python script:  

```python
import logging

# Set up logging
logging.basicConfig(level=logging.DEBUG)  # Setting level to DEBUG
logging.debug("This is a DEBUG message")  # Debugging info
logging.info("This is an INFO message")   # General info
logging.warning("This is a WARNING message")  # Warning
logging.error("This is an ERROR message")  # Error
logging.critical("This is a CRITICAL message")  # Critical issue
```

### **üîπ What will you see in output?**
```
WARNING:root:This is a WARNING message
ERROR:root:This is an ERROR message
CRITICAL:root:This is a CRITICAL message
```
‚ö° **Why no DEBUG or INFO messages?**  
Because by default, logging only **shows WARNING and above**. To include lower levels (`DEBUG`, `INFO`), we set **`level=logging.DEBUG`** in `basicConfig()`.  

---

### **üìå Step 3: Writing Logs to a File (Not Just Console)**
Modify your script to **log into a file**:  

```python
logging.basicConfig(filename="app.log", level=logging.DEBUG, 
                    format="%(asctime)s - %(levelname)s - %(message)s")

logging.info("This will be logged into app.log")
```

üîπ **Now, check `app.log`** file! It should contain something like:  
```
2025-03-13 12:45:00 - INFO - This will be logged into app.log
```

---

### **üìå Step 4: Real-World Use Cases**
#### **‚úÖ 1. Logging in an ML Pipeline (Track Model Training)**
```python
import logging
import time

# Configure logging
logging.basicConfig(filename="ml_training.log", level=logging.INFO, 
                    format="%(asctime)s - %(levelname)s - %(message)s")

logging.info("Starting model training...")

# Simulating training
for epoch in range(1, 6):
    accuracy = 0.80 + (epoch * 0.02)  # Fake accuracy increase
    loss = 0.5 - (epoch * 0.05)  # Fake loss decrease
    logging.info(f"Epoch {epoch}: Accuracy = {accuracy}, Loss = {loss}")
    time.sleep(1)

logging.info("Model training completed!")
```
üìù **Use Case:** If your ML model fails mid-training, logs will help debug issues.  

---

#### **‚úÖ 2. Logging in a Flask API (For Debugging Requests)**
```python
from flask import Flask
import logging

app = Flask(__name__)

# Configure logging
logging.basicConfig(filename="server.log", level=logging.INFO, 
                    format="%(asctime)s - %(levelname)s - %(message)s")

@app.route("/")
def home():
    logging.info("Home page accessed")
    return "Welcome to the Flask API!"

@app.route("/error")
def error():
    logging.error("Error endpoint triggered!")
    return "This is an error!", 500

if __name__ == "__main__":
    app.run(debug=True)
```
üìù **Use Case:** Log API requests and errors for debugging.  

---

### **üìå Step 5: Mini Project - Build a "Smart Logger" for Data Processing**
#### **üõ†Ô∏è Project Idea: Logging in a Data Processing Pipeline**
**Scenario:** You're processing a large CSV file, and you want to **log errors, track progress, and store logs**.  

#### **üîπ What we‚Äôll do:**
‚úÖ Read a **CSV file**  
‚úÖ **Log missing values**  
‚úÖ **Track processing time**  
‚úÖ Save logs to `data_processing.log`  

#### **üîπ Code:**
```python
import logging
import pandas as pd
import time

# Configure logging
logging.basicConfig(filename="data_processing.log", level=logging.INFO, 
                    format="%(asctime)s - %(levelname)s - %(message)s")

def process_csv(file_path):
    try:
        logging.info(f"Starting processing for {file_path}")
        start_time = time.time()

        # Load CSV
        df = pd.read_csv(file_path)

        # Check for missing values
        missing_values = df.isnull().sum().sum()
        if missing_values > 0:
            logging.warning(f"File {file_path} contains {missing_values} missing values")

        # Process data (Dummy operation)
        df.fillna("UNKNOWN", inplace=True)

        end_time = time.time()
        logging.info(f"Processing completed in {end_time - start_time:.2f} seconds")

    except Exception as e:
        logging.error(f"Error processing {file_path}: {str(e)}")

# Run the function
process_csv("data.csv")
```

#### **üîπ How to Test?**
1Ô∏è‚É£ **Create a sample `data.csv` file** with missing values.  
2Ô∏è‚É£ **Run the script** ‚Üí It will log missing values and processing time.  
3Ô∏è‚É£ **Check `data_processing.log`** for saved logs.  

üìù **Use Case:** Useful when handling big datasets in ML projects.  

---

### **üìå Recap & Next Steps**
‚úÖ **Logging Basics** ‚Üí `logging.basicConfig()`, log levels  
‚úÖ **Writing Logs to a File**  
‚úÖ **Real-World Examples** ‚Üí ML pipeline, Flask API  
‚úÖ **Mini Project** ‚Üí Data Processing Logger  


# Logging in Python: Handlers, Rotating Logs, and BackupCount Explained

## 3. Adding Handlers ‚Äî WTF is That?
### Why Do We Need Handlers?
Imagine you want:
- Logs to be saved **in a file** AND displayed in the **terminal (console)**.
- To **limit file size** so logs don‚Äôt get too big.

That‚Äôs where **Handlers** come in.

### Example: Adding a File & Console Handler
```python
import logging

# Create logger
logger = logging.getLogger("MyLogger")
logger.setLevel(logging.INFO)

# File handler (save logs in a file)
file_handler = logging.FileHandler("mylog.log")
file_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))

# Console handler (show logs in terminal)
console_handler = logging.StreamHandler()
console_handler.setFormatter(logging.Formatter("%(levelname)s: %(message)s"))

# Add handlers to logger
logger.addHandler(file_handler)
logger.addHandler(console_handler)

# Example logs
logger.info("This will be saved in a file AND shown in terminal")
logger.warning("Warning message here!")
```

### What Happens?
Logs appear in the terminal like this:
```
INFO: This will be saved in a file AND shown in terminal
WARNING: Warning message here!
```

Logs are also saved in **mylog.log** with timestamps.

üî• Now you can track logs **everywhere**‚Äîboth in the **terminal** and in a **file**!

---

## 4. Rotating Logs ‚Äî WTF is That?
### Why Do We Need Rotating Logs?
Imagine your **log file grows to 1GB** üò±. We don‚Äôt want that! **Rotating logs** ensure that files are rotated **after reaching a certain size**.

### Example: Rotating Logs
```python
from logging.handlers import RotatingFileHandler
import logging

logger = logging.getLogger("MyLogger")
logger.setLevel(logging.INFO)

# Create a rotating file handler (max 5KB, keep 3 old log files)
file_handler = RotatingFileHandler("mylog.log", maxBytes=5000, backupCount=3)
file_handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))

logger.addHandler(file_handler)

# Generate logs
for i in range(1000):
    logger.info(f"This is log message {i}")
```

### What Happens?
- Once **mylog.log reaches 5KB**, it gets renamed to **mylog.log.1**.
- A new empty **mylog.log** is created.
- The process continues, creating **mylog.log.2, mylog.log.3**, etc.
- **Older logs don‚Äôt bloat the disk!**

üî• Now your logs won‚Äôt get out of control.

---

## How `backupCount` and `maxBytes` Work Together
### **Understanding the Rotation Process**
- **`maxBytes=5000`** ‚Üí Once `mylog.log` **reaches 5000 bytes**, it gets renamed to `mylog.log.1` and a new `mylog.log` is created.
- **`backupCount=3`** ‚Üí Keeps **the last 3 rotated logs** (`mylog.log.1`, `mylog.log.2`, `mylog.log.3`) and deletes older ones.
- **New logs are always written to `mylog.log`**, and when it hits `maxBytes`, **rotation happens again**.

---

## Are Rotated Logs the Same Size?
Not **exactly**. Here‚Äôs why:
- `mylog.log` will be **close to** `maxBytes` (~5000 bytes before rotating).
- `mylog.log.1`, `mylog.log.2`, etc., might be **slightly smaller** if rotation happens mid-log message.

### Example Scenario:
1. `mylog.log` ‚Üí **Hits 5000 bytes**.
2. **Rotation happens**:
   - `mylog.log` ‚Üí renamed to `mylog.log.1`.
   - A new empty `mylog.log` is created.
3. **Process repeats** ‚Üí `mylog.log.1` becomes `mylog.log.2`, and `mylog.log.2` becomes `mylog.log.3`.
4. If `mylog.log.3` **already exists**, it **gets deleted** to maintain `backupCount=3`.

---

## Want to Check the File Sizes?
Run this Python script:
```python
import os

log_files = ["mylog.log", "mylog.log.1", "mylog.log.2", "mylog.log.3"]

for log in log_files:
    if os.path.exists(log):
        print(f"{log}: {os.path.getsize(log)} bytes")
```

This will show the **actual size** of each log file!

üî• Now you're in control of **log file sizes** and **rotations**!

